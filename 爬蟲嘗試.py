# -*- coding: utf-8 -*-
"""爬蟲嘗試.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vrJPurwXEbfVvg_vdfgZXf9y8KO1FjOM
"""

!pip install beautifulsoup4

#還未去除的版本
import requests
from bs4 import BeautifulSoup

# 定义维基百科页面的URL
url = 'https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91'

# 发送HTTP请求获取页面内容
response = requests.get(url)

# 使用Beautiful Soup解析HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 找到所有包含段落内容的div元素
paragraph_divs = soup.find_all('div', class_='mw-parser-output')

# 提取每个div元素中的文本内容作为段落
for div in paragraph_divs:
    paragraphs = div.find_all('p')
    for paragraph in paragraphs:
        print(paragraph.text)

#利用get_text()去除HTML標記
import requests
from bs4 import BeautifulSoup

# 定义维基百科页面的URL
url = 'https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91'

# 发送HTTP请求获取页面内容
response = requests.get(url)

# 使用Beautiful Soup解析HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 找到所有包含段落内容的div元素
paragraph_divs = soup.find_all('div', class_='mw-parser-output')

# 提取每个div元素中的文本内容作为段落
for div in paragraph_divs:
    paragraphs = div.find_all('p')
    for paragraph in paragraphs:
        # 去除HTML标记，只保留纯文本内容
        text = paragraph.get_text()
        print(text)

#利用split('\n')來進行分段
import requests
from bs4 import BeautifulSoup

# 定义维基百科页面的URL
url = 'https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91'

# 发送HTTP请求获取页面内容
response = requests.get(url)

# 使用Beautiful Soup解析HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 找到所有包含段落内容的div元素
paragraph_divs = soup.find_all('div', class_='mw-parser-output')

# 提取每个div元素中的文本内容作为段落
for div in paragraph_divs:
    paragraphs = div.find_all('p')
    for paragraph in paragraphs:
        # 去除HTML标记，只保留纯文本内容
        text = paragraph.get_text()
        # 根据换行符分段
        segments = text.split('\n')
        for segment in segments:
            # 如果段落不为空，则打印
            if segment.strip():
                print(segment.strip())

#清洗文本
import requests
from bs4 import BeautifulSoup
import re

# 定义维基百科页面的URL
url = 'https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91'

# 发送HTTP请求获取页面内容
response = requests.get(url)

# 使用Beautiful Soup解析HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 找到所有包含段落内容的div元素
paragraph_divs = soup.find_all('div', class_='mw-parser-output')

# 定义一个函数来清洗文本
def clean_text(text):
    # 去除HTML标记，只保留纯文本内容
    text = re.sub(r'<[^>]+>', '', text)
    # 去除多余的空格和换行符
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# 提取每个div元素中的文本内容作为段落，并进行清洗
for div in paragraph_divs:
    paragraphs = div.find_all('p')
    for paragraph in paragraphs:
        # 获取段落文本并清洗
        cleaned_paragraph = clean_text(paragraph.text)
        # 打印清洗后的段落
        if cleaned_paragraph:
            print(cleaned_paragraph)

#分詞測試
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize

# 示例文本
text = "自然语言处理是人工智能的一个重要领域。"

# 使用NLTK进行分词
tokens = word_tokenize(text)

# 打印分词结果
print(tokens)

import requests
from bs4 import BeautifulSoup
import re
import nltk

# 下载NLTK的punkt分词器
nltk.download('punkt')

# 定义维基百科页面的URL
url = 'https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91'

# 发送HTTP请求获取页面内容
response = requests.get(url)

# 使用Beautiful Soup解析HTML
soup = BeautifulSoup(response.text, 'html.parser')

# 找到所有包含段落内容的div元素
paragraph_divs = soup.find_all('div', class_='mw-parser-output')

# 定义一个函数来清洗文本
def clean_text(text):
    # 去除HTML标记，只保留纯文本内容
    text = re.sub(r'<[^>]+>', '', text)
    # 去除多余的空格和换行符
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# 提取每个div元素中的文本内容作为段落，并进行清洗和分词处理
for div in paragraph_divs:
    paragraphs = div.find_all('p')
    for paragraph in paragraphs:
        # 获取段落文本并清洗
        cleaned_paragraph = clean_text(paragraph.text)
        # 如果清洗后的段落不为空，则进行分词处理
        if cleaned_paragraph:
            # 使用NLTK进行分词
            tokens = nltk.word_tokenize(cleaned_paragraph)
            # 打印分词结果
            print(tokens)