# -*- coding: utf-8 -*-
"""專題.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NYMUgcHCiKEw3X1Rcub1GlU94Za7giCJ
"""

import nltk
from nltk.chat.util import Chat, reflections
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tag import pos_tag
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
import pandas as pd
import os,sys
import jieba

pd.set_option('max_colwidth', 1000)
nltk.download('vader_lexicon')

# 模式匹配
patterns = [
    (r'你好', '嗨！'),
    (r'(.*)天气如何', '我不是气象专家，但你可以查看天气预报。')
]

# 分词
text = "NLTK is a powerful library for AI chat bot"
important_words = nltk.word_tokenize(text)
print(important_words)

# 句子拆分
text = """The sun rose in the clear blue sky, casting its warm rays upon the vibrant green landscape.
Birds chirped their melodious tunes as a gentle breeze rustled the leaves of the trees."""
sentences = sent_tokenize(text)
for sentence in sentences:
    print(sentence)

# 词性标注
words = word_tokenize("NLTK is a powerful library for AI chat bot")
tags = pos_tag(words)
print(tags)

def cutProcess(sting):
    result = jieba.lcut(sting)
    result = " ".join(result)

    return result

df['quote'] = df['quote'].apply(cutProcess)

df.head(5)

model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

input_text = input()
input_ids = tokenizer.encode(input_text, return_tensors="pt")
output = model.generate(input_ids, max_length=50, num_return_sequences=1)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

sid = SentimentIntensityAnalyzer()

sentiment_scores = sid.polarity_scores(input_text)

# 输出情感极性得分
print("Positive score:", sentiment_scores['pos'])
print("Negative score:", sentiment_scores['neg'])
print("Neutral score:", sentiment_scores['neu'])

import re
import json
import random

import nltk
import string
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


def stop_words_and_tokenize(text:str):
    words=word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    punctuation = set(string.punctuation)
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in punctuation]
    ps = PorterStemmer()
    stem_word= map(ps.stem,filtered_words)
    return list(stem_word)

def load_data(file_path:str):
    data= open(file_path,"r")
    data: dict = json.load(data)
    data=data["all_data"]
    return data

def unknown():
    response = ["Could you please re-phrase that? ",
                "Sounds about right.",
                "What does that mean?"][
        random.randrange(3)]
    newknowledge= input("What does it mean? can you teach me: ")
    respon =input("How can I responses : ")
    new_words=stop_words_and_tokenize(prompt_text)
    item={"tag":[newknowledge],"responses": [respon],"patterns":new_words,"keyword":[]}
    config = json.loads(open('data2.json').read())
    config["all_data"].append(item)
    with open('data2.json','w') as f:
        f.write(json.dumps(config,indent=2))
    return("thank you")

def message_probability(user_message, recognised_words, single_response=False, required_words=[]):
    message_certainty = 0
    has_required_words = True

    #計算每條預定義訊息中存在的單字數
    for word in user_message:
        if word in recognised_words:
            message_certainty += 1

    # 計算用戶訊息中已識別單字的百分比
    percentage = float(message_certainty) / float(len(recognised_words))

    #檢查字串中是否包含單字
    for word in required_words:
        if word not in user_message:
            has_required_words = False
            break

    if has_required_words or single_response:
        return int(percentage * 100)
    else:
        return 0


def check_all_messages(message):
    highest_prob_list = {}

    #過濾後將其添加到字典中
    def response(bot_response, list_of_words, single_response=False, required_words=[]):
        nonlocal highest_prob_list
        highest_prob_list[bot_response] = message_probability(message, list_of_words, single_response, required_words)

    n=load_data("data2.json")
    for i in n:
        response(i["responses"][0], i["patterns"], required_words= i["keyword"])

    best_match = max(highest_prob_list, key=highest_prob_list.get)
    print(highest_prob_list)
    print(f'Best match = {best_match} | Score: {highest_prob_list[best_match]}')

    return unknown() if highest_prob_list[best_match] < 1 else best_match

#用於獲取回應
def get_response(user_input):
    split_message = re.split(r'\s+|[,;?!.-]\s*', user_input.lower())
    response = check_all_messages(split_message)
    print(split_message)
    return response

#進行訓練
while True:
    prompt_text=input("You:")
    print(str('Bot: ' + get_response(prompt_text)))